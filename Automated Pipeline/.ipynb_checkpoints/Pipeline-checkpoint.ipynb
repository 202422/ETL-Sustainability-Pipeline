{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dadbb9-51cb-4af1-954d-7886ea0ac417",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install reverse_geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fbf5d-052a-46c2-bd5b-186016312842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from typing import List, Tuple, Optional\n",
    "from sklearn.svm import SVR\n",
    "from typing import List, Tuple, Dict\n",
    "import reverse_geocoder as rg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import FLOAT, TEXT, INTEGER, DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72085fa-ada7-4c4a-8473-4c505af0dd56",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cddf323-1a1f-44c9-bcc7-5c1d8e256700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "class WeatherConfig:\n",
    "    API_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    HOURLY_VARIABLES = [\n",
    "        \"temperature_2m\",\n",
    "        \"relative_humidity_2m\",\n",
    "        \"precipitation\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"weather_code\"\n",
    "    ]\n",
    "    MAX_DAYS_PER_REQUEST = 90\n",
    "    RETRY_CONFIG = {\n",
    "        \"total\": 3,\n",
    "        \"backoff_factor\": 1,\n",
    "        \"status_forcelist\": [429, 500, 502, 503, 504],\n",
    "        \"allowed_methods\": [\"GET\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bec04a-30b8-42b2-9c90-2e765d6e3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HTTP session with retry mechanism\n",
    "def setup_session() -> requests.Session:\n",
    "    \"\"\"Set up and return a session with retry configuration.\"\"\"\n",
    "    session = requests.Session()\n",
    "    adapter = HTTPAdapter(max_retries=Retry(**WeatherConfig.RETRY_CONFIG))\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebd76b-6110-4ce3-b442-0607d59485e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct parameters for API request\n",
    "def build_api_params(latitude: float, longitude: float, start_date: str, end_date: str) -> dict:\n",
    "    \"\"\"Build API request parameters.\"\"\"\n",
    "    return {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \",\".join(WeatherConfig.HOURLY_VARIABLES),\n",
    "        \"timezone\": \"Europe/Paris\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b2d95-89ab-408f-a98e-9b1ed410fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a single chunk of weather data for a specific period\n",
    "def fetch_weather_chunk(\n",
    "    session: requests.Session,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    start_date: datetime,\n",
    "    end_date: datetime\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetch a chunk of weather data for a given period.\"\"\"\n",
    "    try:\n",
    "        params = build_api_params(\n",
    "            latitude,\n",
    "            longitude,\n",
    "            start_date.strftime('%Y-%m-%d'),\n",
    "            end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        response = session.get(WeatherConfig.API_URL, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if \"hourly\" not in data:\n",
    "            print(f\"No hourly data available for {latitude, longitude}\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(data[\"hourly\"])\n",
    "        df[\"latitude\"] = latitude\n",
    "        df[\"longitude\"] = longitude\n",
    "        return df\n",
    "        \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        if response.status_code == 429:\n",
    "            print(f\"Rate limit reached for {latitude, longitude}. Waiting...\")\n",
    "            time.sleep(10)\n",
    "            return None\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"Error 404: Invalid URL or parameters for {latitude, longitude}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"HTTP error for {latitude, longitude}: {str(http_err)}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Connection error for {latitude, longitude}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205b836-e353-41a8-b8ed-bbfbdb21a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all weather data for a single location\n",
    "def fetch_location_data(\n",
    "    session: requests.Session,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    needed_hours: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch all weather data for a given location.\"\"\"\n",
    "    all_data = []\n",
    "    remaining_hours = needed_hours\n",
    "    end_date = datetime.now()\n",
    "    \n",
    "    while remaining_hours > 0:\n",
    "        chunk_days = min(WeatherConfig.MAX_DAYS_PER_REQUEST, (remaining_hours // 24) + 1)\n",
    "        start_date = end_date - timedelta(days=chunk_days)\n",
    "        \n",
    "        chunk_df = fetch_weather_chunk(session, latitude, longitude, start_date, end_date)\n",
    "        \n",
    "        if chunk_df is not None:\n",
    "            all_data.append(chunk_df)\n",
    "            fetched_hours = len(chunk_df)\n",
    "            remaining_hours -= fetched_hours\n",
    "            print(f\"{fetched_hours} hourly records added for {latitude, longitude} (remaining: {remaining_hours})\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        end_date = start_date - timedelta(days=1)\n",
    "        time.sleep(1)  # Respect API rate limits\n",
    "        \n",
    "        if chunk_df is not None and len(chunk_df) == 0:\n",
    "            break\n",
    "    \n",
    "    final_df = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "    return final_df.iloc[:needed_hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d5c0a-0b0f-4178-a714-3bca2eb2a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrate data collection for multiple locations\n",
    "def collect_weather_data(\n",
    "    locations: List[Tuple[float, float]],\n",
    "    needed_hours: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Main pipeline to collect weather data for all locations.\"\"\"\n",
    "    session = setup_session()\n",
    "    all_dfs = []\n",
    "    \n",
    "    for lat, lon in locations:\n",
    "        print(f\"\\n=== Fetching data for location ({lat}, {lon}) ===\")\n",
    "        df_location = fetch_location_data(session, lat, lon, needed_hours)\n",
    "        \n",
    "        if not df_location.empty:\n",
    "            all_dfs.append(df_location)\n",
    "        \n",
    "        time.sleep(2)  # Pause between locations\n",
    "    \n",
    "    final_df = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "    \n",
    "    # Summary\n",
    "    if not final_df.empty:\n",
    "        counts = final_df[\"latitude\"].value_counts()\n",
    "        print(\"\\n=== Summary ===\")\n",
    "        print(f\"Total records retrieved: {len(final_df)}\")\n",
    "        print(\"Breakdown by location:\")\n",
    "        print(counts.to_string())\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f242a95-db6d-4d9d-946e-89b001d524b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "def save_to_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba966a-1ccb-4c82-927d-121eadc340b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the weather data pipeline\n",
    "def main():\n",
    "    \"\"\"Main function to run the weather data pipeline.\"\"\"\n",
    "    LOCATIONS = [\n",
    "        (48.8566, 2.3522),  # Paris\n",
    "        (45.7640, 4.8357),  # Lyon\n",
    "        (43.2965, 5.3698),  # Marseille\n",
    "        (44.8378, -0.5792)  # Bordeaux\n",
    "    ]\n",
    "    NEEDED_HOURS = 3000\n",
    "    \n",
    "    # Run the pipeline\n",
    "    df_meteo = collect_weather_data(LOCATIONS, NEEDED_HOURS)\n",
    "    if not df_meteo.empty:\n",
    "        save_to_csv(df_meteo, \"weather_data_extracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6044d9-e024-4dd5-b0dc-cda97709fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092fb1d-8ae0-425c-af83-d103f1f4670b",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39277f37-b743-49c2-bd11-a3232b7723bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for cleaning pipeline settings\n",
    "class CleaningConfig:\n",
    "    SEQ_SIZE = 72\n",
    "    COLS_TO_IMPUTE = [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"wind_speed_10m\"]\n",
    "    SVR_PARAMS = {\"kernel\": \"linear\", \"gamma\": \"auto\", \"C\": 1.0, \"epsilon\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820a917-cb2d-47fd-b4cd-a8e2821e4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time and weather_code columns to appropriate formats\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert time to datetime and weather_code to category, then sort by time.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean[\"time\"] = pd.to_datetime(df_clean[\"time\"], format=\"%Y-%m-%dT%H:%M\")\n",
    "    df_clean[\"weather_code\"] = df_clean[\"weather_code\"].astype(\"category\")\n",
    "    df_clean.sort_values(by=\"time\", ascending=False, inplace=True, ignore_index=True)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75302a0b-b189-4208-b8f7-d26f9421dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows to keep only the first third of rows with missing values\n",
    "def filter_missing_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Keep only the first third of rows with missing values and all non-missing rows.\"\"\"\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    n = len(nan_rows)\n",
    "    first_third_nan_rows = nan_rows.iloc[:n // 3]\n",
    "    return pd.concat([\n",
    "        df[~df.isna().any(axis=1)],  # Rows without NaN\n",
    "        first_third_nan_rows         # First third of rows with NaN\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab28e-498d-4f2b-8e8b-4b31de0f427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into separate dataframes by location\n",
    "def split_by_location(df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"Split dataframe into separate dataframes based on unique latitude-longitude pairs.\"\"\"\n",
    "    locs = df[[\"latitude\", \"longitude\"]].drop_duplicates().values\n",
    "    return [\n",
    "        df[(df[\"latitude\"] == lat) & (df[\"longitude\"] == lon)].copy()\n",
    "        for lat, lon in locs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c58e3-6e05-49f0-9b9b-01d19ef4585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data into sequences for SVR training\n",
    "def to_sequence(data: pd.Series, seq_size: int = CleaningConfig.SEQ_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Convert a time series into sequences for SVR training.\"\"\"\n",
    "    x, y = [], []\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(data) - seq_size):\n",
    "        window = data[i:(i + seq_size)].values\n",
    "        after_window = data[i + seq_size]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "    \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf95b8c-1166-4e4f-b571-7fed81ae556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVR models for numerical columns\n",
    "def train_svr_models(df: pd.DataFrame, cols_to_impute: List[str]) -> dict:\n",
    "    \"\"\"Train SVR models for each numerical column to impute missing values.\"\"\"\n",
    "    models = {}\n",
    "    for col in cols_to_impute:\n",
    "        print(f\"Processing column {col}\")\n",
    "        series = df[col].dropna()\n",
    "        X, y = to_sequence(series, seq_size=CleaningConfig.SEQ_SIZE)\n",
    "        \n",
    "        model = SVR(**CleaningConfig.SVR_PARAMS)\n",
    "        print(\"Training model...\")\n",
    "        model.fit(X, y)\n",
    "        print(\"Training completed.\")\n",
    "        \n",
    "        models[col] = model\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b65a00-e369-469b-8091-57fc38c48563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in a dataframe using SVR and mode for weather_code\n",
    "def impute_dataset(df: pd.DataFrame, cols_to_impute: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Impute numerical columns with SVR models and weather_code with mode.\"\"\"\n",
    "    df_imputed = df.copy().reset_index(drop=True)\n",
    "    models = train_svr_models(df_imputed, cols_to_impute)\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        print(f\"\\n=== Imputing column: {col} ===\")\n",
    "        model = models[col]\n",
    "        nan_indices = df_imputed[df_imputed[col].isna()].index\n",
    "        \n",
    "        for idx in nan_indices:\n",
    "            start_idx = idx - CleaningConfig.SEQ_SIZE\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "                \n",
    "            window = df_imputed.loc[start_idx:idx - 1, col]\n",
    "            if window.isna().any() or len(window) != CleaningConfig.SEQ_SIZE:\n",
    "                continue\n",
    "                \n",
    "            input_seq = window.values.reshape(1, -1)\n",
    "            prediction = model.predict(input_seq)[0]\n",
    "            df_imputed.at[idx, col] = prediction\n",
    "    \n",
    "    if \"weather_code\" in df_imputed.columns:\n",
    "        mode_weather = df_imputed[\"weather_code\"].mode()[0]\n",
    "        df_imputed[\"weather_code\"] = df_imputed[\"weather_code\"].fillna(mode_weather)\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a72872-a549-49e4-89b4-80ca2d0487be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all location dataframes and combine results\n",
    "def process_locations(dfs: List[pd.DataFrame], cols_to_impute: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing values for each location dataframe and combine results.\"\"\"\n",
    "    imputed_dfs = []\n",
    "    \n",
    "    for i, df in enumerate(dfs, start=1):\n",
    "        print(f\"\\n=== Processing dataset df_loc{i} ===\")\n",
    "        df_imputed = impute_dataset(df, cols_to_impute)\n",
    "        imputed_dfs.append(df_imputed)\n",
    "    \n",
    "    final_df = pd.concat(imputed_dfs, ignore_index=True)\n",
    "    final_df.sort_values(by=\"time\", ascending=False, inplace=True, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a6f7c-94ef-4738-add3-31e7cc609fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cleaned dataframe to CSV\n",
    "def save_cleaned_data(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save the cleaned dataframe to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Cleaned data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe29ac-14a7-478c-8d46-3a36b90220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline for cleaning weather data\n",
    "def clean_weather_data(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute the complete cleaning pipeline for weather data.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df_clean = preprocess_dataframe(df)\n",
    "    \n",
    "    # Filter missing rows\n",
    "    df_filtered = filter_missing_rows(df_clean)\n",
    "    \n",
    "    # Split by location\n",
    "    location_dfs = split_by_location(df_filtered)\n",
    "    \n",
    "    # Process and impute all locations\n",
    "    final_df = process_locations(location_dfs, CleaningConfig.COLS_TO_IMPUTE)\n",
    "    \n",
    "    # Save results\n",
    "    save_cleaned_data(final_df, output_file)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e460d7d-f778-4f51-bbc7-ce96b8a3fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the cleaning pipeline\n",
    "def main():\n",
    "    \"\"\"Run the weather data cleaning pipeline.\"\"\"\n",
    "    INPUT_FILE = \"weather_data_extracted.csv\"\n",
    "    OUTPUT_FILE = \"weather_data_cleaned.csv\"\n",
    "    \n",
    "    clean_weather_data(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05a492-8bed-4355-8f15-777460b7ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536bac95-68f2-4aa8-b9cb-00ae239dc61a",
   "metadata": {},
   "source": [
    "# Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9ae2b-5d8a-4afb-8294-5ed156227d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for transformation pipeline settings\n",
    "class TransformConfig:\n",
    "    COLUMNS_TO_NORMALIZE = [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"wind_speed_10m\", \"heat_index\"]\n",
    "    TIME_BINS = [0, 6, 12, 18, 24]\n",
    "    TIME_LABELS = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\"]\n",
    "    AGGREGATION_DICT = {\n",
    "        \"temperature_2m\": \"mean\",\n",
    "        \"relative_humidity_2m\": \"mean\",\n",
    "        \"precipitation\": \"mean\",\n",
    "        \"wind_speed_10m\": \"mean\",\n",
    "        \"weather_code\": lambda x: x.mode()[0],\n",
    "        \"location\": \"first\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd599-39ea-493e-ba88-f054387a9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe by converting column types\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert time to datetime and weather_code to category.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean[\"time\"] = pd.to_datetime(df_clean[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_clean[\"weather_code\"] = df_clean[\"weather_code\"].astype(\"category\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ea1fd-1db2-4b32-a04d-485fa152da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform reverse geocoding and create location dataframe\n",
    "def add_location_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add location names to the dataframe using reverse geocoding.\"\"\"\n",
    "    locs = df[[\"latitude\", \"longitude\"]].drop_duplicates().values\n",
    "    coordinates = [(lat, lon) for lat, lon in locs]\n",
    "    results = rg.search(coordinates)\n",
    "    \n",
    "    location_data = [\n",
    "        {\"latitude\": lat, \"longitude\": lon, \"location\": f\"{res['name']}\"}\n",
    "        for (lat, lon), res in zip(locs, results)\n",
    "    ]\n",
    "    location_df = pd.DataFrame(location_data)\n",
    "    \n",
    "    df_with_locations = df.merge(location_df, on=[\"latitude\", \"longitude\"], how=\"left\")\n",
    "    df_with_locations[\"time\"] = pd.to_datetime(df_with_locations[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_with_locations[\"weather_code\"] = df_with_locations[\"weather_code\"].astype(\"category\")\n",
    "    df_with_locations[\"location\"] = df_with_locations[\"location\"].astype(\"category\")\n",
    "    \n",
    "    return df_with_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a765355-3dd8-4a2c-9ccd-442826e85b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by time of day and location\n",
    "def aggregate_by_time_of_day(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform data by aggregating based on time of day and location.\"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed[\"time_of_day\"] = pd.cut(\n",
    "        df_transformed[\"time\"].dt.hour,\n",
    "        bins=TransformConfig.TIME_BINS,\n",
    "        labels=TransformConfig.TIME_LABELS,\n",
    "        right=False,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    df_transformed[\"date\"] = df_transformed[\"time\"].dt.date\n",
    "    \n",
    "    result = df_transformed.groupby(\n",
    "        [\"latitude\", \"longitude\", \"date\", \"time_of_day\"],\n",
    "        observed=True\n",
    "    ).agg(TransformConfig.AGGREGATION_DICT).reset_index()\n",
    "    \n",
    "    result[\"time_of_day\"] = result[\"time_of_day\"].astype(\"category\")\n",
    "    result[\"weather_code\"] = result[\"weather_code\"].astype(\"category\")\n",
    "    result[\"location\"] = result[\"location\"].astype(\"category\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e55d5-6658-4f4a-be7f-c50a16da3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate heat index for each row\n",
    "def calculate_heat_index(row: pd.Series) -> float:\n",
    "    \"\"\"Calculate heat index based on temperature and relative humidity.\"\"\"\n",
    "    T = row[\"temperature_2m\"]\n",
    "    RH = row[\"relative_humidity_2m\"]\n",
    "    if T >= 20:\n",
    "        return T + 0.33 * RH - 0.7\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927db22-9195-4f3f-b7b1-b86dbb4e4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heat index and raining indicator columns\n",
    "def add_derived_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add heat index and is_raining columns to the dataframe.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    df_features[\"heat_index\"] = df_features.apply(calculate_heat_index, axis=1)\n",
    "    df_features[\"is_raining\"] = (df_features[\"precipitation\"] > 0).astype(int)\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60fc4a-3d16-4b67-8539-182701d6d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize specified columns using StandardScaler\n",
    "def normalize_data(df: pd.DataFrame, columns_to_normalize: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Normalize specified columns using StandardScaler (Z-score).\"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    missing_cols = [col for col in columns_to_normalize if col not in df_normalized.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    print(\"\\nBefore normalization:\")\n",
    "    print(df_normalized[columns_to_normalize].describe())\n",
    "    \n",
    "    df_normalized[columns_to_normalize] = scaler.fit_transform(df_normalized[columns_to_normalize])\n",
    "    \n",
    "    print(\"\\nAfter normalization:\")\n",
    "    print(df_normalized[columns_to_normalize].describe())\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12304d79-ea1b-400c-b807-9034de251eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed dataframe to CSV\n",
    "def save_transformed_data(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save the transformed dataframe to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Transformed data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf31b73-301b-484e-8c6a-6292d926e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline for transforming weather data\n",
    "def transform_weather_data(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute the complete transformation pipeline for weather data.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df_clean = preprocess_dataframe(df)\n",
    "    \n",
    "    # Add location data\n",
    "    df_with_locations = add_location_data(df_clean)\n",
    "    \n",
    "    # Aggregate by time of day\n",
    "    df_time_of_day = aggregate_by_time_of_day(df_with_locations)\n",
    "    \n",
    "    # Add derived features\n",
    "    df_features = add_derived_features(df_time_of_day)\n",
    "    \n",
    "    # Normalize data\n",
    "    df_normalized = normalize_data(df_features, TransformConfig.COLUMNS_TO_NORMALIZE)\n",
    "    \n",
    "    # Save results\n",
    "    save_transformed_data(df_normalized, output_file)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178d4a9-a4bb-451a-89f2-0ec27edff14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the transformation pipeline\n",
    "def main():\n",
    "    \"\"\"Run the weather data transformation pipeline.\"\"\"\n",
    "    INPUT_FILE = \"weather_data_cleaned.csv\"\n",
    "    OUTPUT_FILE = \"weather_data_transformed.csv\"\n",
    "    \n",
    "    transform_weather_data(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0b50f-9f78-4acf-bf75-2b8e3083884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abc0e3-8a6e-4494-9fdb-15813fbed3dd",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3419ae-daa8-4baa-a718-0478825389f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for database pipeline settings\n",
    "class DatabaseConfig:\n",
    "    DATABASE_NAME = \"weather_database.db\"\n",
    "    LOCATIONS_TABLE = \"Locations\"\n",
    "    OBSERVATIONS_TABLE = \"Weather_Observations\"\n",
    "    WEATHER_COLUMNS = [\n",
    "        \"location_id\", \"date\", \"time_of_day\", \"temperature_2m\", \"relative_humidity_2m\",\n",
    "        \"precipitation\", \"wind_speed_10m\", \"weather_code\", \"heat_index\", \"is_raining\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96719e65-864b-43d7-b81b-7d7be4cd6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe by converting column types\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns to appropriate data types for database storage.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean[\"time_of_day\"] = df_clean[\"time_of_day\"].astype(\"category\")\n",
    "    df_clean[\"location\"] = df_clean[\"location\"].astype(\"category\")\n",
    "    df_clean[\"weather_code\"] = df_clean[\"weather_code\"].astype(str)  # Convert float64 to string\n",
    "    df_clean[\"date\"] = pd.to_datetime(df_clean[\"date\"]).dt.date  # Convert to datetime.date\n",
    "    df_clean[\"is_raining\"] = df_clean[\"is_raining\"].astype(\"int32\")  # Optimize to int32\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef2a3f-b7b0-4445-96bc-baa8f3f90fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database tables\n",
    "def create_database_tables(conn: sqlite3.Connection) -> None:\n",
    "    \"\"\"Create Locations and Weather_Observations tables in the SQLite database.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {DatabaseConfig.LOCATIONS_TABLE} (\n",
    "            location_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            latitude FLOAT NOT NULL,\n",
    "            longitude FLOAT NOT NULL,\n",
    "            location TEXT NOT NULL\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {DatabaseConfig.OBSERVATIONS_TABLE} (\n",
    "            observation_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            location_id INTEGER NOT NULL,\n",
    "            date DATE NOT NULL,\n",
    "            time_of_day TEXT NOT NULL,\n",
    "            temperature_2m FLOAT NOT NULL,\n",
    "            relative_humidity_2m FLOAT NOT NULL,\n",
    "            precipitation FLOAT NOT NULL,\n",
    "            wind_speed_10m FLOAT NOT NULL,\n",
    "            weather_code TEXT NOT NULL,\n",
    "            heat_index FLOAT NOT NULL,\n",
    "            is_raining INTEGER NOT NULL,\n",
    "            FOREIGN KEY (location_id) REFERENCES {DatabaseConfig.LOCATIONS_TABLE}(location_id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53511f4d-907f-43ef-9606-6fc1c77356e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert unique locations into the Locations table\n",
    "def insert_locations(df: pd.DataFrame, engine: sqlalchemy.engine.Engine) -> Dict[Tuple[float, float], int]:\n",
    "    \"\"\"Insert unique locations into the Locations table and return a location map.\"\"\"\n",
    "    locations = df[[\"latitude\", \"longitude\", \"location\"]].drop_duplicates()\n",
    "    locations.to_sql(\n",
    "        DatabaseConfig.LOCATIONS_TABLE,\n",
    "        engine,\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        dtype={\"latitude\": FLOAT, \"longitude\": FLOAT, \"location\": TEXT}\n",
    "    )\n",
    "    \n",
    "    # Create a location map (latitude, longitude) -> location_id\n",
    "    location_df = pd.read_sql_query(f\"SELECT * FROM {DatabaseConfig.LOCATIONS_TABLE}\", engine)\n",
    "    location_map = {\n",
    "        (row[\"latitude\"], row[\"longitude\"]): row[\"location_id\"]\n",
    "        for _, row in location_df.iterrows()\n",
    "    }\n",
    "    \n",
    "    return location_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46f00a-ceff-4b0a-bdd5-7905461e8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add location_id to the dataframe\n",
    "def add_location_id(df: pd.DataFrame, location_map: Dict[Tuple[float, float], int]) -> pd.DataFrame:\n",
    "    \"\"\"Add location_id column to the dataframe using the location map.\"\"\"\n",
    "    df_with_location_id = df.copy()\n",
    "    df_with_location_id[\"location_id\"] = df_with_location_id.apply(\n",
    "        lambda row: location_map.get((row[\"latitude\"], row[\"longitude\"])), axis=1\n",
    "    )\n",
    "    return df_with_location_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c137ae-b473-4ba9-9d41-ee2c50d2865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert weather observations into the Weather_Observations table\n",
    "def insert_weather_observations(df: pd.DataFrame, engine: sqlalchemy.engine.Engine) -> None:\n",
    "    \"\"\"Insert weather observations into the Weather_Observations table.\"\"\"\n",
    "    df[DatabaseConfig.WEATHER_COLUMNS].to_sql(\n",
    "        DatabaseConfig.OBSERVATIONS_TABLE,\n",
    "        engine,\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        dtype={\n",
    "            \"location_id\": INTEGER,\n",
    "            \"date\": DATE,\n",
    "            \"time_of_day\": TEXT,\n",
    "            \"temperature_2m\": FLOAT,\n",
    "            \"relative_humidity_2m\": FLOAT,\n",
    "            \"precipitation\": FLOAT,\n",
    "            \"wind_speed_10m\": FLOAT,\n",
    "            \"weather_code\": TEXT,\n",
    "            \"heat_index\": FLOAT,\n",
    "            \"is_raining\": INTEGER\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662b4bd-c6f7-460f-926c-61a437bf7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data from database tables\n",
    "def display_sample_data(conn: sqlite3.Connection) -> None:\n",
    "    \"\"\"Display the first 5 rows from Locations and Weather_Observations tables.\"\"\"\n",
    "    print(\"Locations:\")\n",
    "    print(pd.read_sql_query(f\"SELECT * FROM {DatabaseConfig.LOCATIONS_TABLE} LIMIT 5\", conn))\n",
    "    print(\"\\nWeather Observations:\")\n",
    "    print(pd.read_sql_query(f\"SELECT * FROM {DatabaseConfig.OBSERVATIONS_TABLE} LIMIT 5\", conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b27cdc-cbd4-466d-b22a-1a287ed4671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline for storing weather data in SQLite database\n",
    "def store_weather_data(input_file: str) -> None:\n",
    "    \"\"\"Execute the complete pipeline for storing weather data in a SQLite database.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df_clean = preprocess_dataframe(df)\n",
    "    \n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(DatabaseConfig.DATABASE_NAME)\n",
    "    engine = create_engine(f\"sqlite:///{DatabaseConfig.DATABASE_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        # Create tables\n",
    "        create_database_tables(conn)\n",
    "        \n",
    "        # Insert locations and get location map\n",
    "        location_map = insert_locations(df_clean, engine)\n",
    "        \n",
    "        # Add location_id to dataframe\n",
    "        df_with_location_id = add_location_id(df_clean, location_map)\n",
    "        \n",
    "        # Insert weather observations\n",
    "        insert_weather_observations(df_with_location_id, engine)\n",
    "        \n",
    "        # Display sample data\n",
    "        display_sample_data(conn)\n",
    "        \n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd502f-7d87-4123-b48e-c95d13bdd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the database storage pipeline\n",
    "def main():\n",
    "    \"\"\"Run the weather data storage pipeline.\"\"\"\n",
    "    INPUT_FILE = \"weather_data_transformed.csv\"\n",
    "    store_weather_data(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ff186-0436-45cf-884e-f45d06e6f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
