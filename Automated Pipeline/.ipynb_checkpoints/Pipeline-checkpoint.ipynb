{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dadbb9-51cb-4af1-954d-7886ea0ac417",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install reverse_geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fbf5d-052a-46c2-bd5b-186016312842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from typing import List, Tuple, Optional\n",
    "from sklearn.svm import SVR\n",
    "from typing import List, Tuple, Dict\n",
    "import reverse_geocoder as rg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine, Column, Float, Integer, String, ForeignKey\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72085fa-ada7-4c4a-8473-4c505af0dd56",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cddf323-1a1f-44c9-bcc7-5c1d8e256700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "class WeatherConfig:\n",
    "    API_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    HOURLY_VARIABLES = [\n",
    "        \"temperature_2m\",\n",
    "        \"relative_humidity_2m\",\n",
    "        \"precipitation\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"weather_code\"\n",
    "    ]\n",
    "    MAX_DAYS_PER_REQUEST = 90\n",
    "    RETRY_CONFIG = {\n",
    "        \"total\": 3,\n",
    "        \"backoff_factor\": 1,\n",
    "        \"status_forcelist\": [429, 500, 502, 503, 504],\n",
    "        \"allowed_methods\": [\"GET\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bec04a-30b8-42b2-9c90-2e765d6e3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HTTP session with retry mechanism\n",
    "def setup_session() -> requests.Session:\n",
    "    \"\"\"Set up and return a session with retry configuration.\"\"\"\n",
    "    session = requests.Session()\n",
    "    adapter = HTTPAdapter(max_retries=Retry(**WeatherConfig.RETRY_CONFIG))\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebd76b-6110-4ce3-b442-0607d59485e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct parameters for API request\n",
    "def build_api_params(latitude: float, longitude: float, start_date: str, end_date: str) -> dict:\n",
    "    \"\"\"Build API request parameters.\"\"\"\n",
    "    return {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \",\".join(WeatherConfig.HOURLY_VARIABLES),\n",
    "        \"timezone\": \"Europe/Paris\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b2d95-89ab-408f-a98e-9b1ed410fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a single chunk of weather data for a specific period\n",
    "def fetch_weather_chunk(\n",
    "    session: requests.Session,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    start_date: datetime,\n",
    "    end_date: datetime\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetch a chunk of weather data for a given period.\"\"\"\n",
    "    try:\n",
    "        params = build_api_params(\n",
    "            latitude,\n",
    "            longitude,\n",
    "            start_date.strftime('%Y-%m-%d'),\n",
    "            end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        response = session.get(WeatherConfig.API_URL, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if \"hourly\" not in data:\n",
    "            print(f\"No hourly data available for {latitude, longitude}\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(data[\"hourly\"])\n",
    "        df[\"latitude\"] = latitude\n",
    "        df[\"longitude\"] = longitude\n",
    "        return df\n",
    "        \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        if response.status_code == 429:\n",
    "            print(f\"Rate limit reached for {latitude, longitude}. Waiting...\")\n",
    "            time.sleep(10)\n",
    "            return None\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"Error 404: Invalid URL or parameters for {latitude, longitude}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"HTTP error for {latitude, longitude}: {str(http_err)}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Connection error for {latitude, longitude}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205b836-e353-41a8-b8ed-bbfbdb21a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all weather data for a single location\n",
    "def fetch_location_data(\n",
    "    session: requests.Session,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    needed_hours: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch all weather data for a given location.\"\"\"\n",
    "    all_data = []\n",
    "    remaining_hours = needed_hours\n",
    "    end_date = datetime.now()\n",
    "    \n",
    "    while remaining_hours > 0:\n",
    "        chunk_days = min(WeatherConfig.MAX_DAYS_PER_REQUEST, (remaining_hours // 24) + 1)\n",
    "        start_date = end_date - timedelta(days=chunk_days)\n",
    "        \n",
    "        chunk_df = fetch_weather_chunk(session, latitude, longitude, start_date, end_date)\n",
    "        \n",
    "        if chunk_df is not None:\n",
    "            all_data.append(chunk_df)\n",
    "            fetched_hours = len(chunk_df)\n",
    "            remaining_hours -= fetched_hours\n",
    "            print(f\"{fetched_hours} hourly records added for {latitude, longitude} (remaining: {remaining_hours})\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        end_date = start_date - timedelta(days=1)\n",
    "        time.sleep(1)  # Respect API rate limits\n",
    "        \n",
    "        if chunk_df is not None and len(chunk_df) == 0:\n",
    "            break\n",
    "    \n",
    "    final_df = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "    return final_df.iloc[:needed_hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d5c0a-0b0f-4178-a714-3bca2eb2a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrate data collection for multiple locations\n",
    "def collect_weather_data(\n",
    "    locations: List[Tuple[float, float]],\n",
    "    needed_hours: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Main pipeline to collect weather data for all locations.\"\"\"\n",
    "    session = setup_session()\n",
    "    all_dfs = []\n",
    "    \n",
    "    for lat, lon in locations:\n",
    "        print(f\"\\n=== Fetching data for location ({lat}, {lon}) ===\")\n",
    "        df_location = fetch_location_data(session, lat, lon, needed_hours)\n",
    "        \n",
    "        if not df_location.empty:\n",
    "            all_dfs.append(df_location)\n",
    "        \n",
    "        time.sleep(2)  # Pause between locations\n",
    "    \n",
    "    final_df = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "    \n",
    "    # Summary\n",
    "    if not final_df.empty:\n",
    "        counts = final_df[\"latitude\"].value_counts()\n",
    "        print(\"\\n=== Summary ===\")\n",
    "        print(f\"Total records retrieved: {len(final_df)}\")\n",
    "        print(\"Breakdown by location:\")\n",
    "        print(counts.to_string())\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f242a95-db6d-4d9d-946e-89b001d524b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "def save_to_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba966a-1ccb-4c82-927d-121eadc340b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the weather data pipeline\n",
    "def main():\n",
    "    \"\"\"Main function to run the weather data pipeline.\"\"\"\n",
    "    LOCATIONS = [\n",
    "        (48.8566, 2.3522),  # Paris\n",
    "        (45.7640, 4.8357),  # Lyon\n",
    "        (43.2965, 5.3698),  # Marseille\n",
    "        (44.8378, -0.5792)  # Bordeaux\n",
    "    ]\n",
    "    NEEDED_HOURS = 3000\n",
    "    \n",
    "    # Run the pipeline\n",
    "    df_meteo = collect_weather_data(LOCATIONS, NEEDED_HOURS)\n",
    "    if not df_meteo.empty:\n",
    "        save_to_csv(df_meteo, \"weather_data_extracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6044d9-e024-4dd5-b0dc-cda97709fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092fb1d-8ae0-425c-af83-d103f1f4670b",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39277f37-b743-49c2-bd11-a3232b7723bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for cleaning pipeline settings\n",
    "class CleaningConfig:\n",
    "    SEQ_SIZE = 72\n",
    "    COLS_TO_IMPUTE = [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"wind_speed_10m\"]\n",
    "    SVR_PARAMS = {\"kernel\": \"linear\", \"gamma\": \"auto\", \"C\": 1.0, \"epsilon\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820a917-cb2d-47fd-b4cd-a8e2821e4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time and weather_code columns to appropriate formats\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert time to datetime and weather_code to category, then sort by time.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean[\"time\"] = pd.to_datetime(df_clean[\"time\"], format=\"%Y-%m-%dT%H:%M\")\n",
    "    df_clean[\"weather_code\"] = df_clean[\"weather_code\"].astype(\"category\")\n",
    "    df_clean.sort_values(by=\"time\", ascending=False, inplace=True, ignore_index=True)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75302a0b-b189-4208-b8f7-d26f9421dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows to keep only the first third of rows with missing values\n",
    "def filter_missing_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Keep only the first third of rows with missing values and all non-missing rows.\"\"\"\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    n = len(nan_rows)\n",
    "    first_third_nan_rows = nan_rows.iloc[:n // 3]\n",
    "    return pd.concat([\n",
    "        df[~df.isna().any(axis=1)],  # Rows without NaN\n",
    "        first_third_nan_rows         # First third of rows with NaN\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab28e-498d-4f2b-8e8b-4b31de0f427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into separate dataframes by location\n",
    "def split_by_location(df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"Split dataframe into separate dataframes based on unique latitude-longitude pairs.\"\"\"\n",
    "    locs = df[[\"latitude\", \"longitude\"]].drop_duplicates().values\n",
    "    return [\n",
    "        df[(df[\"latitude\"] == lat) & (df[\"longitude\"] == lon)].copy()\n",
    "        for lat, lon in locs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c58e3-6e05-49f0-9b9b-01d19ef4585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data into sequences for SVR training\n",
    "def to_sequence(data: pd.Series, seq_size: int = CleaningConfig.SEQ_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Convert a time series into sequences for SVR training.\"\"\"\n",
    "    x, y = [], []\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(data) - seq_size):\n",
    "        window = data[i:(i + seq_size)].values\n",
    "        after_window = data[i + seq_size]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "    \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf95b8c-1166-4e4f-b571-7fed81ae556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVR models for numerical columns\n",
    "def train_svr_models(df: pd.DataFrame, cols_to_impute: List[str]) -> dict:\n",
    "    \"\"\"Train SVR models for each numerical column to impute missing values.\"\"\"\n",
    "    models = {}\n",
    "    for col in cols_to_impute:\n",
    "        print(f\"Processing column {col}\")\n",
    "        series = df[col].dropna()\n",
    "        X, y = to_sequence(series, seq_size=CleaningConfig.SEQ_SIZE)\n",
    "        \n",
    "        model = SVR(**CleaningConfig.SVR_PARAMS)\n",
    "        print(\"Training model...\")\n",
    "        model.fit(X, y)\n",
    "        print(\"Training completed.\")\n",
    "        \n",
    "        models[col] = model\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b65a00-e369-469b-8091-57fc38c48563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in a dataframe using SVR and mode for weather_code\n",
    "def impute_dataset(df: pd.DataFrame, cols_to_impute: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Impute numerical columns with SVR models and weather_code with mode.\"\"\"\n",
    "    df_imputed = df.copy().reset_index(drop=True)\n",
    "    models = train_svr_models(df_imputed, cols_to_impute)\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        print(f\"\\n=== Imputing column: {col} ===\")\n",
    "        model = models[col]\n",
    "        nan_indices = df_imputed[df_imputed[col].isna()].index\n",
    "        \n",
    "        for idx in nan_indices:\n",
    "            start_idx = idx - CleaningConfig.SEQ_SIZE\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "                \n",
    "            window = df_imputed.loc[start_idx:idx - 1, col]\n",
    "            if window.isna().any() or len(window) != CleaningConfig.SEQ_SIZE:\n",
    "                continue\n",
    "                \n",
    "            input_seq = window.values.reshape(1, -1)\n",
    "            prediction = model.predict(input_seq)[0]\n",
    "            df_imputed.at[idx, col] = prediction\n",
    "    \n",
    "    if \"weather_code\" in df_imputed.columns:\n",
    "        mode_weather = df_imputed[\"weather_code\"].mode()[0]\n",
    "        df_imputed[\"weather_code\"] = df_imputed[\"weather_code\"].fillna(mode_weather)\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a72872-a549-49e4-89b4-80ca2d0487be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all location dataframes and combine results\n",
    "def process_locations(dfs: List[pd.DataFrame], cols_to_impute: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing values for each location dataframe and combine results.\"\"\"\n",
    "    imputed_dfs = []\n",
    "    \n",
    "    for i, df in enumerate(dfs, start=1):\n",
    "        print(f\"\\n=== Processing dataset df_loc{i} ===\")\n",
    "        df_imputed = impute_dataset(df, cols_to_impute)\n",
    "        imputed_dfs.append(df_imputed)\n",
    "    \n",
    "    final_df = pd.concat(imputed_dfs, ignore_index=True)\n",
    "    final_df.sort_values(by=\"time\", ascending=False, inplace=True, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a6f7c-94ef-4738-add3-31e7cc609fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cleaned dataframe to CSV\n",
    "def save_cleaned_data(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save the cleaned dataframe to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Cleaned data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe29ac-14a7-478c-8d46-3a36b90220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline for cleaning weather data\n",
    "def clean_weather_data(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute the complete cleaning pipeline for weather data.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df_clean = preprocess_dataframe(df)\n",
    "    \n",
    "    # Filter missing rows\n",
    "    df_filtered = filter_missing_rows(df_clean)\n",
    "    \n",
    "    # Split by location\n",
    "    location_dfs = split_by_location(df_filtered)\n",
    "    \n",
    "    # Process and impute all locations\n",
    "    final_df = process_locations(location_dfs, CleaningConfig.COLS_TO_IMPUTE)\n",
    "    \n",
    "    # Save results\n",
    "    save_cleaned_data(final_df, output_file)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e460d7d-f778-4f51-bbc7-ce96b8a3fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the cleaning pipeline\n",
    "def main():\n",
    "    \"\"\"Run the weather data cleaning pipeline.\"\"\"\n",
    "    INPUT_FILE = \"weather_data_extracted.csv\"\n",
    "    OUTPUT_FILE = \"weather_data_cleaned.csv\"\n",
    "    \n",
    "    clean_weather_data(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05a492-8bed-4355-8f15-777460b7ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536bac95-68f2-4aa8-b9cb-00ae239dc61a",
   "metadata": {},
   "source": [
    "# Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9ae2b-5d8a-4afb-8294-5ed156227d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for transformation pipeline settings\n",
    "class TransformConfig:\n",
    "    COLUMNS_TO_NORMALIZE = [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"wind_speed_10m\", \"heat_index\"]\n",
    "    TIME_BINS = [0, 6, 12, 18, 24]\n",
    "    TIME_LABELS = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\"]\n",
    "    AGGREGATION_DICT = {\n",
    "        \"temperature_2m\": \"mean\",\n",
    "        \"relative_humidity_2m\": \"mean\",\n",
    "        \"precipitation\": \"mean\",\n",
    "        \"wind_speed_10m\": \"mean\",\n",
    "        \"weather_code\": lambda x: x.mode()[0],\n",
    "        \"location\": \"first\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd599-39ea-493e-ba88-f054387a9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe by converting column types\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert time to datetime and weather_code to category.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean[\"time\"] = pd.to_datetime(df_clean[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_clean[\"weather_code\"] = df_clean[\"weather_code\"].astype(\"category\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ea1fd-1db2-4b32-a04d-485fa152da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform reverse geocoding and create location dataframe\n",
    "def add_location_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add location names to the dataframe using reverse geocoding.\"\"\"\n",
    "    locs = df[[\"latitude\", \"longitude\"]].drop_duplicates().values\n",
    "    coordinates = [(lat, lon) for lat, lon in locs]\n",
    "    results = rg.search(coordinates)\n",
    "    \n",
    "    location_data = [\n",
    "        {\"latitude\": lat, \"longitude\": lon, \"location\": f\"{res['name']}\"}\n",
    "        for (lat, lon), res in zip(locs, results)\n",
    "    ]\n",
    "    location_df = pd.DataFrame(location_data)\n",
    "    \n",
    "    df_with_locations = df.merge(location_df, on=[\"latitude\", \"longitude\"], how=\"left\")\n",
    "    df_with_locations[\"time\"] = pd.to_datetime(df_with_locations[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_with_locations[\"weather_code\"] = df_with_locations[\"weather_code\"].astype(\"category\")\n",
    "    df_with_locations[\"location\"] = df_with_locations[\"location\"].astype(\"category\")\n",
    "    \n",
    "    return df_with_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a765355-3dd8-4a2c-9ccd-442826e85b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by time of day and location\n",
    "def aggregate_by_time_of_day(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform data by aggregating based on time of day and location.\"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed[\"time_of_day\"] = pd.cut(\n",
    "        df_transformed[\"time\"].dt.hour,\n",
    "        bins=TransformConfig.TIME_BINS,\n",
    "        labels=TransformConfig.TIME_LABELS,\n",
    "        right=False,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    df_transformed[\"date\"] = df_transformed[\"time\"].dt.date\n",
    "    \n",
    "    result = df_transformed.groupby(\n",
    "        [\"latitude\", \"longitude\", \"date\", \"time_of_day\"],\n",
    "        observed=True\n",
    "    ).agg(TransformConfig.AGGREGATION_DICT).reset_index()\n",
    "    \n",
    "    result[\"time_of_day\"] = result[\"time_of_day\"].astype(\"category\")\n",
    "    result[\"weather_code\"] = result[\"weather_code\"].astype(\"category\")\n",
    "    result[\"location\"] = result[\"location\"].astype(\"category\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e55d5-6658-4f4a-be7f-c50a16da3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate heat index for each row\n",
    "def calculate_heat_index(row: pd.Series) -> float:\n",
    "    \"\"\"Calculate heat index based on temperature and relative humidity.\"\"\"\n",
    "    T = row[\"temperature_2m\"]\n",
    "    RH = row[\"relative_humidity_2m\"]\n",
    "    if T >= 20:\n",
    "        return T + 0.33 * RH - 0.7\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927db22-9195-4f3f-b7b1-b86dbb4e4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heat index and raining indicator columns\n",
    "def add_derived_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add heat index and is_raining columns to the dataframe.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    df_features[\"heat_index\"] = df_features.apply(calculate_heat_index, axis=1)\n",
    "    df_features[\"is_raining\"] = (df_features[\"precipitation\"] > 0).astype(int)\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60fc4a-3d16-4b67-8539-182701d6d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize specified columns using StandardScaler\n",
    "def normalize_data(df: pd.DataFrame, columns_to_normalize: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Normalize specified columns using StandardScaler (Z-score).\"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    missing_cols = [col for col in columns_to_normalize if col not in df_normalized.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    print(\"\\nBefore normalization:\")\n",
    "    print(df_normalized[columns_to_normalize].describe())\n",
    "    \n",
    "    df_normalized[columns_to_normalize] = scaler.fit_transform(df_normalized[columns_to_normalize])\n",
    "    \n",
    "    print(\"\\nAfter normalization:\")\n",
    "    print(df_normalized[columns_to_normalize].describe())\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12304d79-ea1b-400c-b807-9034de251eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed dataframe to CSV\n",
    "def save_transformed_data(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save the transformed dataframe to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Transformed data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf31b73-301b-484e-8c6a-6292d926e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline for transforming weather data\n",
    "def transform_weather_data(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute the complete transformation pipeline for weather data.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df_clean = preprocess_dataframe(df)\n",
    "    \n",
    "    # Add location data\n",
    "    df_with_locations = add_location_data(df_clean)\n",
    "    \n",
    "    # Aggregate by time of day\n",
    "    df_time_of_day = aggregate_by_time_of_day(df_with_locations)\n",
    "    \n",
    "    # Add derived features\n",
    "    df_features = add_derived_features(df_time_of_day)\n",
    "    \n",
    "    # Normalize data\n",
    "    df_normalized = normalize_data(df_features, TransformConfig.COLUMNS_TO_NORMALIZE)\n",
    "    \n",
    "    # Save results\n",
    "    save_transformed_data(df_normalized, output_file)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178d4a9-a4bb-451a-89f2-0ec27edff14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for the transformation pipeline\n",
    "def main():\n",
    "    \"\"\"Run the weather data transformation pipeline.\"\"\"\n",
    "    INPUT_FILE = \"weather_data_cleaned.csv\"\n",
    "    OUTPUT_FILE = \"weather_data_transformed.csv\"\n",
    "    \n",
    "    transform_weather_data(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0b50f-9f78-4acf-bf75-2b8e3083884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abc0e3-8a6e-4494-9fdb-15813fbed3dd",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac312c0-508f-46d8-8f67-0afa24bf71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models(Base):\n",
    "    \"\"\"Define SQLAlchemy models for Location, Time, and WeatherMeasurement.\"\"\"\n",
    "    class Location(Base):\n",
    "        __tablename__ = 'location'\n",
    "        id = Column(Integer, primary_key=True)\n",
    "        latitude = Column(Float)\n",
    "        longitude = Column(Float)\n",
    "        location_name = Column(String)\n",
    "        weather_data = relationship(\"WeatherMeasurement\", back_populates=\"location\")\n",
    "\n",
    "    class Time(Base):\n",
    "        __tablename__ = 'time'\n",
    "        id = Column(Integer, primary_key=True)\n",
    "        date = Column(String)\n",
    "        time_of_day = Column(String)\n",
    "        weather_data = relationship(\"WeatherMeasurement\", back_populates=\"time\")\n",
    "\n",
    "    class WeatherMeasurement(Base):\n",
    "        __tablename__ = 'weather_measurement'\n",
    "        id = Column(Integer, primary_key=True)\n",
    "        location_id = Column(Integer, ForeignKey('location.id'))\n",
    "        time_id = Column(Integer, ForeignKey('time.id'))\n",
    "        temperature_2m = Column(Float)\n",
    "        relative_humidity_2m = Column(Float)\n",
    "        precipitation = Column(Float)\n",
    "        wind_speed_10m = Column(Float)\n",
    "        weather_code = Column(Float)\n",
    "        heat_index = Column(Float)\n",
    "        is_raining = Column(Integer)\n",
    "        location = relationship(\"Location\", back_populates=\"weather_data\")\n",
    "        time = relationship(\"Time\", back_populates=\"weather_data\")\n",
    "    \n",
    "    return Location, Time, WeatherMeasurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08ac48-6065-452a-b096-26539edb7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database(db_name):\n",
    "    \"\"\"Set up SQLite database and return engine and session.\"\"\"\n",
    "    engine = create_engine(f'sqlite:///{db_name}')\n",
    "    Base = declarative_base()\n",
    "    Location, Time, WeatherMeasurement = define_models(Base)\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    return engine, session, Location, Time, WeatherMeasurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e21216-d97e-476e-b6fd-3124976673e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, session, Location, Time, WeatherMeasurement):\n",
    "    \"\"\"Process CSV data and insert into database.\"\"\"\n",
    "    location_cache = {}\n",
    "    time_cache = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Location\n",
    "        loc_key = (row['latitude'], row['longitude'], row['location'])\n",
    "        if loc_key not in location_cache:\n",
    "            loc = Location(latitude=row['latitude'], longitude=row['longitude'], \n",
    "                         location_name=row['location'])\n",
    "            session.add(loc)\n",
    "            session.flush()\n",
    "            location_cache[loc_key] = loc.id\n",
    "        location_id = location_cache[loc_key]\n",
    "\n",
    "        # Time\n",
    "        time_key = (row['date'], row['time_of_day'])\n",
    "        if time_key not in time_cache:\n",
    "            time = Time(date=row['date'], time_of_day=row['time_of_day'])\n",
    "            session.add(time)\n",
    "            session.flush()\n",
    "            time_cache[time_key] = time.id\n",
    "        time_id = time_cache[time_key]\n",
    "\n",
    "        # WeatherMeasurement\n",
    "        weather = WeatherMeasurement(\n",
    "            location_id=location_id,\n",
    "            time_id=time_id,\n",
    "            temperature_2m=row['temperature_2m'],\n",
    "            relative_humidity_2m=row['relative_humidity_2m'],\n",
    "            precipitation=row['precipitation'],\n",
    "            wind_speed_10m=row['wind_speed_10m'],\n",
    "            weather_code=row['weather_code'],\n",
    "            heat_index=row['heat_index'],\n",
    "            is_raining=row['is_raining']\n",
    "        )\n",
    "        session.add(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6f30b-7427-4ad6-a2b2-735b6b800807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(session, WeatherMeasurement, limit):\n",
    "    \"\"\"Retrieve and display weather measurement records.\"\"\"\n",
    "    results = session.query(WeatherMeasurement).limit(limit).all()\n",
    "    for measurement in results:\n",
    "        print(\"---\")\n",
    "        print(f\"Date: {measurement.time.date} {measurement.time.time_of_day}\")\n",
    "        print(f\"Location: {measurement.location.location_name} \"\n",
    "              f\"({measurement.location.latitude}, {measurement.location.longitude})\")\n",
    "        print(f\"Temperature: {measurement.temperature_2m} °C\")\n",
    "        print(f\"Humidity: {measurement.relative_humidity_2m} %\")\n",
    "        print(f\"Precipitation: {measurement.precipitation} mm\")\n",
    "        print(f\"Wind Speed: {measurement.wind_speed_10m} m/s\")\n",
    "        print(f\"Weather Code: {measurement.weather_code}\")\n",
    "        print(f\"Heat Index: {measurement.heat_index} °C\")\n",
    "        print(f\"Is Raining: {'Yes' if measurement.is_raining else 'No'}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599ac0c-c586-4080-8313-2f6679ef8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_weather_database(csv_path, db_name, limit=5):\n",
    "    \"\"\"\n",
    "    Manages weather data operations by calling modular functions.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file\n",
    "        db_name (str): Name of the SQLite database\n",
    "        limit (int): Number of records to retrieve and display (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of retrieved WeatherMeasurement objects\n",
    "    \"\"\"\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Setup database\n",
    "    engine, session, Location, Time, WeatherMeasurement = setup_database(db_name)\n",
    "    \n",
    "    # Process data\n",
    "    process_data(df, session, Location, Time, WeatherMeasurement)\n",
    "    \n",
    "    # Commit changes\n",
    "    session.commit()\n",
    "    \n",
    "    # Display results\n",
    "    results = display_results(session, WeatherMeasurement, limit)\n",
    "    \n",
    "    # Close session\n",
    "    session.close()\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
